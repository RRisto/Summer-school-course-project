{
    "contents" : "########################################################################\n#################functions for scraping metadata and articles\n#helper function to make NYT API call url\nmakeURL <- function(q=NULL, fq=NULL, begin_date=NULL, end_date=NULL, \n                    key=apikey, page=0, sort=NULL, fl=NULL, \n                    hl=NULL, facet_field=NULL,facet_filter=NULL){\n  #input= argumenst of NYT API call, more about arguments read here: \n  #https://developer.nytimes.com/article_search_v2.json#/README\n  #output (character string)= url for NYT API call\n  arglist <- list(q=q, fq=fq, begin_date=begin_date, end_date=end_date, \n                  'api-key'=key, page=page, sort=sort, fl=fl, hl=hl,\n                  facet_field=facet_field,\n                  facet_filter=facet_filter)\n  url <- paste0('http://api.nytimes.com/svc/search/v2/articlesearch.json?')\n  for(i in 1:length(arglist)){\n    if(is.null(unlist(arglist[i]))==F){\n      url <- paste0(url, '&', names(arglist[i]), '=', arglist[i])\n    }\n  }\n  return(url)\n}\n\n#small helper function to make section names for query \nmakeFq=function(section){\n  #https://developer.nytimes.com/article_search_v2.json#/README\n  #input (character)= section name(s) (double qoutation!): '\"Sports\", \"Arts\"'\n  #output (character)= fq variable for getMetaData function.\n  paste0('section_name:(', section,')')\n}\n\n#function that gets meta data of specified number of articles\n#takes them as as dayStep specified chunks \n#(max 1000 articles per day for given filters)\ngetMetaData=function(apikey,nrOfArticles=300, beginDate=\"20160619\", \n                     backward=T, sort=NULL,fq =NULL, fl=NULL, \n                     hl=NULL, facet_field=NULL, dayStep=1,\n                     facet_filter=NULL) {\n  #input:\n  #apikey (character)=your key for NYT API.\n  #nrOfArticles (integer)=nr of articles which metadata you want to scrape. \n  #beginDate (character, format: YYYYMMDD)=date from which the scraping begins. \n  #backward (boolean)=how scraping is done considering time scale: from begin \n  #date to future or to the past, deafult to the past. \n  #section (character, use function makeFq)=section which articles you want \n  #to scrape rest of the arguments are from function makeURL\n  #dayStep (integer) = from how big period chunks articles will be taken.\n  #by default will take daily, if articles are widely distributed in time \n  #it is reasonable to take bigger chunks\n  #output: dataframe with following columns: urls, section_names, titles,\n  #dates\n  require(jsonlite)\n  library(RCurl)\n  #initial sanity check, if there are some articles in sepcified nr of loops,\n  #and asks user what to do\n  beginDateinside=beginDate\n  endDateinside=beginDate\n  if(backward==T) {\n    #date from which begin (max period based on daystep and API call limit)\n    checkDate=gsub(\"-\",\"\",as.Date(strptime(beginDateinside, \n                                           \"%Y%m%d\"))-(999*dayStep))\n    checkurl=makeURL(fq =fq,begin_date=checkDate, end_date=endDateinside,\n                     sort=sort, fl=fl, hl=hl, facet_field=facet_field,\n                     facet_filter=facet_filter)\n  } else {\n    checkDate=gsub(\"-\",\"\",as.Date(strptime(beginDateinside, \n                                           \"%Y%m%d\"))+(999*dayStep))\n    checkurl=makeURL(fq =fq,begin_date=beginDateinside, end_date=checkDate,\n                     sort=sort, fl=fl, hl=hl, facet_field=facet_field,\n                     facet_filter=facet_filter)\n  }\n  print(checkurl)\n  #nr of articles for this period\n  hits=fromJSON(txt=URLencode(checkurl), flatten = T)$response$meta$hits\n  #if hits is smaller than nrOfAticles, than stop\n  if(hits<nrOfArticles) {\n    stop(paste0(\"There are \", hits,\" articles, but you wanted \",nrOfArticles, \n                \". Please specify nrOfArticles/beginData/dayStep\"))\n  }\n  # set up initial user choiche which is False\n  userChoice=\"I have no idea\"\n  #ask user choice\n  while (!userChoice%in%c(\"1\", \"0\")) {\n    userChoice=readline(prompt=paste0(hits, \" hits from \", checkDate, \" to \", \n                                      beginDate,\". To continue press 1,\n                                      to stop press 0: \"))\n  }\n  #act according to user choice\n  if (userChoice==\"0\") {\n    stop(\"Stopping this session\") #stop session\n  } else if (userChoice==\"1\") {\n    cat(\"Let's rock and roll \\n\") #continue\n  }\n  \n  #######function main body\n  #initialize variables to where we loop information needed\n  urls=c()\n  section_names=c()\n  titles=c()\n  dates=c()\n  callCounter=1#counts nr of all calls to avoid infinite loops and API limit\n  #start looping, second condition is stupid way to escape infinite loop\n  while(length(urls)<nrOfArticles&&callCounter<=999) {\n    #initialise dates between which we search for articles\n    if(backward==T) {\n      endDateinside=gsub(\"-\",\"\",\n                         as.Date(strptime(beginDateinside, \"%Y%m%d\"))-1)\n      beginDateinside=gsub(\"-\",\"\",\n                           as.Date(strptime(beginDateinside, \"%Y%m%d\"))-\n                             dayStep)\n    } else {\n      beginDateinside=gsub(\"-\",\"\",as.Date(strptime(endDateinside, \"%Y%m%d\"))+\n                             1)\n      endDateinside=gsub(\"-\",\"\",as.Date(strptime(endDateinside, \"%Y%m%d\"))+\n                           dayStep)\n    }\n    #when error occurs display message and continue\n    tryget <- try({\n      #initial nr of hits\n      hits=0\n      callTry=1#nr of tries to display the number for user\n      #loop until find page that has non-0 hits\n      #second condition is stupid way to escape infinite loop\n      while(hits==0&&callCounter<=999) {\n        initcall=URLencode(makeURL(fq =fq,begin_date=beginDateinside, \n                                end_date=endDateinside,\n                                sort=sort, fl=fl, hl=hl, \n                                facet_field=facet_field,\n                                facet_filter=facet_filter))\n        #nr of articles in query\n        hits=fromJSON(txt=URLencode(initcall), flatten = T)$response$meta$hits\n        callCounter=callCounter+1\n        cat(\"Looking for calls that have some hits. Try nr \", \n            callTry, \" \\n\")\n        callTry=callTry+1\n      }\n      #nr of loops needed for that query page where there are at leats 1 hits\n      #(max 99, because max 100 pages (starting from 0!) are allowed\n      #by NYT API per unique url\n      nloops=min(max(ceiling(hits/10-1),0), 99)\n      #loop meta data from pages\n      for(i in 0:nloops) {#start looping from 0 because page nr start from 0\n        if(length(urls)<nrOfArticles) {\n          url=makeURL(fq =fq,page=i,sort=sort, fl=fl, hl=hl,\n                      facet_field=facet_field,\n                      facet_filter=facet_filter,\n                      begin_date=beginDateinside, \n                      end_date=endDateinside)\n          response=fromJSON(txt=URLencode(url), flatten = T)\n          #append data into our vectors        \n          urls=append(urls, response$response$docs$web_url)\n          section_names=append(section_names,response$response$docs$section_name)\n          titles=append(titles, response$response$docs$headline.main)\n          dates=append(dates,response$response$docs$pub_date )\n          #display message for user\n          cat(\"Metadata for\", length(urls), \"articles. API call nr \",\n              callCounter,  \" \\n\")\n          #prints also link, needed for debugging\n          print(makeURL(fq =fq,page=i,sort=sort, fl=fl, hl=hl, \n                        facet_field=facet_field,facet_filter=facet_filter,\n                        begin_date=beginDateinside, end_date=endDateinside))\n          callCounter=callCounter+1\n          Sys.sleep(0.2) #not ot make too many calls (5 per second are allowed)\n        } else {\n          #maybe I can delete this\n          results=data.frame(urls, section_names, titles,dates)\n        }\n      }\n    }) #end of tryget\n    #if some error happened, give message with url and continue\n    if(class(tryget)=='try-error') { \n      cat(\"page number:\",length(urls)+1, ' error - body not scraped, url:',\n          makeURL(fq =fq,page=i,sort=sort, fl=fl, hl=hl, \n                  facet_field=facet_field,facet_filter=facet_filter,\n                  begin_date=beginDateinside, end_date=endDateinside),'\\n')\n      #write NAs to vairables we are scraping\n      urls[i]=NA\n      section_names[i]=NA\n      titles[i]=NA\n      dates[i]=NA\n      next\n    }\n    callCounter=callCounter+1\n  }\n  results=data.frame(urls, section_names, titles,dates)\n  #display results  \n  results\n  }\n\n#function to get article body from metadata scraped\ngetArticleBody=function(articleUrls, \n                        selector=c('article > div', '.articleBody')) {\n  #input: \n  #articleUrls (character vector): article urls from metadata via NYT API \n  #selector (character vector)= selectors which indicate articles body, might\n  #be more, needs to be tested to find out\n  #output (character vector)=article bodys\n  library(rvest)\n  body=c()#initialize vector where we add article body\n  for (i in 1:length(articleUrls)) {\n    url=as.character(articleUrls[i])\n    \n    tryget <- try({ #is needed if some error happens, then it continues\n      page=read_html(url)\n      for (j in 1:length(selector)) {\n        if(length(page %>% #if response has 0 characters of body\n                  html_nodes(selector[j]) %>%\n                  html_text())==0) {\n          body[i]=NA\n        } else {\n          body[i]=paste(page %>% #paste needed to collapse char vec into 1 vec\n                          html_nodes(selector[j]) %>%\n                          html_text(), collapse = '')\n          break #if body found no need to try other selectors\n        }\n      }\n      cat(\"Worked on article nr\",paste0(i, \",\"),\n          paste0(\"progress: \",round(i/length(articleUrls)*100,3),\"%\"),\"\\n\")\n    })\n    #if some error occured, give message and continue\n    if(class(tryget)=='try-error') { \n      cat(\"page number:\",i, ' error - body not scraped \\n')\n      body[i]=NA\n      next\n    }\n  }\n  body\n}",
    "created" : 1468349089966.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "430937004",
    "id" : "5FD9EED1",
    "lastKnownWriteTime" : 1468354466,
    "path" : "~/GitHub/Summer-school-course-project/helpers.R",
    "project_path" : "helpers.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_source"
}